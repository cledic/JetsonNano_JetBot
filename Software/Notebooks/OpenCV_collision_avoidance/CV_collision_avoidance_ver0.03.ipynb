{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La Cam da OpenCV deve essere usate tramite GStreamer con un comando dedicato\n",
    "# https://stackoverflow.com/questions/64272731/open-cv-shows-green-screen-on-jetson-nano\n",
    "#\n",
    "# Eseguire :\n",
    "# sudo nvpmodel -m 0\n",
    "# sudo jetson_clocks\n",
    "# per portare a 4 le CPU operative. OpenCV su NVIDIA Jetson Nano non è ottimizzato per usare la GPU\n",
    "# Il codice gira tutto su CPU con una certa lentezza.\n",
    "#\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output, display, Image\n",
    "import ipywidgets\n",
    "import ipywidgets.widgets as widgets\n",
    "import time\n",
    "from datetime import datetime\n",
    "#\n",
    "from jetbot import Robot\n",
    "#\n",
    "# https://stackoverflow.com/questions/71205664/kill-a-loop-with-button-jupyter-notebook\n",
    "# sudo -H pip install jupyter-ui-poll\n",
    "from jupyter_ui_poll import ui_events\n",
    "from ipywidgets import Button\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot = Robot()\n",
    "robot.motor_driver._pwm.setPWMFreq(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_chunks function creats chunks.\n",
    "# inputs -- EdgeArray and the size_of_chunk to create.\n",
    "# output -- yield successive n-sized chunks.\n",
    "def make_chunks(EdgeArray, size_of_chunk):\n",
    "    chunks = []\n",
    "    for i in range(0, len(EdgeArray), size_of_chunk):\n",
    "        chunks.append(EdgeArray[i:i + size_of_chunk])\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# https://github.com/JetsonHacksNano/CSI-Camera/blob/master/simple_camera.py\n",
    "#\n",
    "\n",
    "# MIT License\n",
    "# Copyright (c) 2019-2022 JetsonHacks\n",
    "\n",
    "# Using a CSI camera (such as the Raspberry Pi Version 2) connected to a\n",
    "# NVIDIA Jetson Nano Developer Kit using OpenCV\n",
    "# Drivers for the camera and OpenCV are included in the base image\n",
    "\n",
    "import cv2\n",
    "\n",
    "\"\"\" \n",
    "gstreamer_pipeline returns a GStreamer pipeline for capturing from the CSI camera\n",
    "Flip the image by setting the flip_method (most common values: 0 and 2)\n",
    "display_width and display_height determine the size of each camera pane in the window on the screen\n",
    "Default 1920x1080 displayd in a 1/4 size window\n",
    "\"\"\"\n",
    "\n",
    "def gstreamer_pipeline(\n",
    "    sensor_id=0,\n",
    "    capture_width=640,\n",
    "    capture_height=480,\n",
    "    display_width=640,\n",
    "    display_height=480,\n",
    "    framerate=20,\n",
    "    flip_method=0,\n",
    "):\n",
    "    return (\n",
    "        \"nvarguscamerasrc sensor-id=%d !\"\n",
    "        \"video/x-raw(memory:NVMM), width=(int)%d, height=(int)%d, framerate=(fraction)%d/1 ! \"\n",
    "        \"nvvidconv flip-method=%d ! \"\n",
    "        \"video/x-raw, width=(int)%d, height=(int)%d, format=(string)BGRx ! \"\n",
    "        \"videoconvert ! \"\n",
    "        \"video/x-raw, format=(string)BGR ! appsink\"\n",
    "        % (\n",
    "            sensor_id,\n",
    "            capture_width,\n",
    "            capture_height,\n",
    "            framerate,\n",
    "            flip_method,\n",
    "            display_width,\n",
    "            display_height,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop(change):\n",
    "    robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_passed(oldepoch):\n",
    "    return time.time() - oldepoch >= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ui_done = False\n",
    "\n",
    "def on_click(btn):\n",
    "    global ui_done\n",
    "    ui_done = True\n",
    "    btn.description = 'STOPPED!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ab22f89afe47b3b9598b9d1afea291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='STOP!', style=ButtonStyle()), Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# To display images to verify. Set to zero if not intrested in displaying images.\n",
    "robot_cam = 1\n",
    "# \n",
    "runningmode = 0\n",
    "#\n",
    "robot_speed = 0.15\n",
    "robot_pause = 0\n",
    "robot_dir   = 0\n",
    "#\n",
    "ui_done = False\n",
    "\n",
    "btn = Button(description='STOP!')\n",
    "btn.on_click(on_click)\n",
    "\n",
    "if robot_cam == 1:\n",
    "    display_handle=display(None, display_id=True)\n",
    "    navigation_w = ipywidgets.Image(format='jpeg')\n",
    "    slider_container = widgets.HBox([btn, navigation_w])\n",
    "else:\n",
    "    display(btn)\n",
    "\n",
    "# Code to draw edge representation.\n",
    "\n",
    "# cap = cv2.VideoCapture(0)  # Creating object to capturing frame from inbult camera(0) or the external camera(1)\n",
    "cap = cv2.VideoCapture(gstreamer_pipeline(flip_method=0), cv2.CAP_GSTREAMER)\n",
    "    \n",
    "StepSize = 5\n",
    "actual_time = time.time()\n",
    "\n",
    "with ui_events() as poll:\n",
    "    while ui_done is False:\n",
    "        poll(10)          # React to UI events (upto 10 at a time)\n",
    "        start = datetime.now()\n",
    "        _, frame = cap.read()  # Reading the frame from the object.\n",
    "\n",
    "        original_frame = frame.copy()  # Copy of frame which will be used for to compare with other images after appling various operations.\n",
    "        img_edgerep = frame.copy()  # Copy of frame which will be used for edge representation.\n",
    "        img_contour = frame.copy()  # Copy of frame which will be used for drawing contours.\n",
    "        img_navigation = frame.copy()  # Copy of frame which will be used for indicating direction of navigation.\n",
    "\n",
    "        blur = cv2.bilateralFilter(img_edgerep, 9, 40, 40)  # Blurring the image to remove the noise present in the image.\n",
    "        edges = cv2.Canny(blur, 50, 100)  # Obtaining clear edges using canny edge detector.\n",
    "\n",
    "        img_edgerep_h = img_edgerep.shape[0] - 1  # Storing the height of the image which will be used in for loop.\n",
    "        img_edgerep_w = img_edgerep.shape[1] - 1  # Storing the width of the image which will be used in for loop.\n",
    "\n",
    "        EdgeArray = []  # Initilizing the array to store the concerned edges for edge representation.\n",
    "\n",
    "        for j in range(0, img_edgerep_w, StepSize):  # FOR loop along the width of the image with given stepsize.\n",
    "            pixel = (j, 0)  # If no edge found in column this value will be stored in edgearray.\n",
    "            for i in range(img_edgerep_h - 5, 0, -1):  # FOR loop along the height of the image.\n",
    "                if edges.item(i, j) == 255:  # Checking for edges.\n",
    "                    pixel = (j, i)\n",
    "                    break  # If edge is found break and go for the next colomn.\n",
    "            EdgeArray.append(pixel)  # Store the eged detected in EgdeArray.\n",
    "\n",
    "        for x in range(len(\n",
    "                EdgeArray) - 1):  # Joining each edge to diferentiate the frame into free space and conjusted space(with objects)\n",
    "            cv2.line(img_edgerep, EdgeArray[x], EdgeArray[x + 1], (0, 255, 0), 1)\n",
    "\n",
    "        for x in range(len(\n",
    "                EdgeArray)):  # Joining each point in the EdgeArray to the respective bottom edge of the frame to represent free space for the bot to move around\n",
    "            cv2.line(img_edgerep, (x * StepSize, img_edgerep_h), EdgeArray[x], (0, 255, 0), 1)\n",
    "\n",
    "        # Code to draw contours.\n",
    "\n",
    "        blurred_frame = cv2.bilateralFilter(img_contour, 9, 75, 75)\n",
    "        gray = cv2.cvtColor(blurred_frame, cv2.COLOR_BGR2GRAY)\n",
    "        ret, thresh = cv2.threshold(gray, 106, 255, 1)\n",
    "        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(img_edgerep, contours, -1, (0, 0, 255), 3)\n",
    "\n",
    "        # Code to decide direction of navigation\n",
    "\n",
    "        number_of_chunks = 3\n",
    "        size_of_chunk = int(len(EdgeArray) / number_of_chunks)\n",
    "        chunks = make_chunks(EdgeArray, size_of_chunk)  # Calling make_chunks function to create the chunks.\n",
    "        avg_of_chunk = []\n",
    "        for i in range(len(chunks) - 1):\n",
    "            x_vals = []\n",
    "            y_vals = []\n",
    "            for (x, y) in chunks[i]:  # Storing the x and y value saperatly to find average.\n",
    "                x_vals.append(x)\n",
    "                y_vals.append(y)\n",
    "            avg_x = int(np.average(x_vals))\n",
    "            avg_y = int(np.average(y_vals))\n",
    "            avg_of_chunk.append([avg_y, avg_x])\n",
    "            cv2.line(frame, (int(img_edgerep_w / 2), img_edgerep_h), (avg_x, avg_y), (255, 0, 0),\n",
    "                         2)  # Draw lines to each average chunks to decide the direction of navigation.\n",
    "\n",
    "        forwardEdge = avg_of_chunk[1]\n",
    "        cv2.line(frame, (int(img_edgerep_w / 2), img_edgerep_h), (forwardEdge[1], forwardEdge[0]), (0, 255, 0), 3)\n",
    "        farthest_point = (min(avg_of_chunk))\n",
    "        # print(farthest_point)\n",
    "\n",
    "        end = datetime.now()\n",
    "        time_taken = end - start\n",
    "        \n",
    "        direction = \"STOP\"\n",
    "        \n",
    "        if forwardEdge[0] > 250:  # Checking for the object at the front is close to bot.\n",
    "            if farthest_point[1] < 310:  # Checking for the farthest_point on the left of the frame.\n",
    "                direction = \"Move left \"\n",
    "                if runningmode == 1:\n",
    "                    robot.left(robot_speed)\n",
    "                    time.sleep(robot_pause)\n",
    "            else:\n",
    "                direction = \"Move right \"\n",
    "                if runningmode == 1:\n",
    "                    robot.right(robot_speed)\n",
    "                    time.sleep(robot_pause)\n",
    "        else:\n",
    "            direction = \"Move forward \"\n",
    "            if runningmode == 1:\n",
    "                robot.forward(robot_speed)\n",
    "                time.sleep(robot_pause)\n",
    "        \n",
    "        if robot_dir == 1:\n",
    "            print(direction+\" \",time_taken)\n",
    "        \n",
    "            # Code to display the results\n",
    "        if robot_cam == 1 and seconds_passed(actual_time):\n",
    "            actual_time = time.time()\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            navigation = cv2.putText(frame, direction, (50, 55), font, 3, (0, 255, 0), 3, cv2.LINE_AA)\n",
    "            clear_output(wait=True)            \n",
    "            lines, columns, _ =  navigation.shape\n",
    "            navigation = cv2.resize(navigation, (int(columns/4), int(lines/4)))\n",
    "            navigation_w.value =cv2.imencode('.jpeg', navigation)[1].tobytes()\n",
    "            display(slider_container)\n",
    "        \n",
    "robot.stop()        \n",
    "cap.release()\n",
    "if robot_cam == 1:\n",
    "    display_handle.update(None)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
